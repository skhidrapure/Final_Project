{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a57a3729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "# Part 1 - Data Preprocessing\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# Importing the training set\n",
    "dataset= pd.read_csv(\"Datasets/SPX_Health_Sector_stocks_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2c5b09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Date'] =  pd.to_datetime(dataset['Date'], infer_datetime_format=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5ab7fbe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Trading Volume</th>\n",
       "      <th>Volume Weighted Average Price</th>\n",
       "      <th>Number of Transactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABT</td>\n",
       "      <td>2020-03-11</td>\n",
       "      <td>73.28</td>\n",
       "      <td>78.15</td>\n",
       "      <td>72.0100</td>\n",
       "      <td>74.74</td>\n",
       "      <td>14123240.0</td>\n",
       "      <td>74.9395</td>\n",
       "      <td>113015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABT</td>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>77.91</td>\n",
       "      <td>82.67</td>\n",
       "      <td>74.8300</td>\n",
       "      <td>81.65</td>\n",
       "      <td>18521541.0</td>\n",
       "      <td>77.7958</td>\n",
       "      <td>160884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABT</td>\n",
       "      <td>2020-03-15</td>\n",
       "      <td>70.65</td>\n",
       "      <td>79.25</td>\n",
       "      <td>70.0000</td>\n",
       "      <td>73.66</td>\n",
       "      <td>14358978.0</td>\n",
       "      <td>75.4936</td>\n",
       "      <td>113523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABT</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>75.52</td>\n",
       "      <td>79.79</td>\n",
       "      <td>73.3400</td>\n",
       "      <td>79.49</td>\n",
       "      <td>13441141.0</td>\n",
       "      <td>77.9694</td>\n",
       "      <td>124950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABT</td>\n",
       "      <td>2020-03-17</td>\n",
       "      <td>74.98</td>\n",
       "      <td>81.99</td>\n",
       "      <td>74.3900</td>\n",
       "      <td>79.26</td>\n",
       "      <td>17680823.0</td>\n",
       "      <td>78.7228</td>\n",
       "      <td>141919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29878</th>\n",
       "      <td>BNTX</td>\n",
       "      <td>2020-12-23</td>\n",
       "      <td>98.50</td>\n",
       "      <td>99.89</td>\n",
       "      <td>96.6400</td>\n",
       "      <td>96.96</td>\n",
       "      <td>1039109.0</td>\n",
       "      <td>97.8215</td>\n",
       "      <td>14211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29879</th>\n",
       "      <td>BNTX</td>\n",
       "      <td>2020-12-27</td>\n",
       "      <td>95.00</td>\n",
       "      <td>95.40</td>\n",
       "      <td>87.0700</td>\n",
       "      <td>88.11</td>\n",
       "      <td>4698872.0</td>\n",
       "      <td>89.9943</td>\n",
       "      <td>58940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29880</th>\n",
       "      <td>BNTX</td>\n",
       "      <td>2020-12-28</td>\n",
       "      <td>84.45</td>\n",
       "      <td>89.39</td>\n",
       "      <td>83.8525</td>\n",
       "      <td>88.28</td>\n",
       "      <td>3300440.0</td>\n",
       "      <td>87.1904</td>\n",
       "      <td>35970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29881</th>\n",
       "      <td>BNTX</td>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>90.00</td>\n",
       "      <td>90.89</td>\n",
       "      <td>83.0268</td>\n",
       "      <td>83.94</td>\n",
       "      <td>2941083.0</td>\n",
       "      <td>85.8906</td>\n",
       "      <td>35970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29882</th>\n",
       "      <td>BNTX</td>\n",
       "      <td>2020-12-30</td>\n",
       "      <td>83.38</td>\n",
       "      <td>85.50</td>\n",
       "      <td>80.5500</td>\n",
       "      <td>81.52</td>\n",
       "      <td>2607147.0</td>\n",
       "      <td>82.1550</td>\n",
       "      <td>31521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13355 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Ticker       Date   Open   High      Low  Close  Trading Volume  \\\n",
       "0        ABT 2020-03-11  73.28  78.15  72.0100  74.74      14123240.0   \n",
       "1        ABT 2020-03-12  77.91  82.67  74.8300  81.65      18521541.0   \n",
       "2        ABT 2020-03-15  70.65  79.25  70.0000  73.66      14358978.0   \n",
       "3        ABT 2020-03-16  75.52  79.79  73.3400  79.49      13441141.0   \n",
       "4        ABT 2020-03-17  74.98  81.99  74.3900  79.26      17680823.0   \n",
       "...      ...        ...    ...    ...      ...    ...             ...   \n",
       "29878   BNTX 2020-12-23  98.50  99.89  96.6400  96.96       1039109.0   \n",
       "29879   BNTX 2020-12-27  95.00  95.40  87.0700  88.11       4698872.0   \n",
       "29880   BNTX 2020-12-28  84.45  89.39  83.8525  88.28       3300440.0   \n",
       "29881   BNTX 2020-12-29  90.00  90.89  83.0268  83.94       2941083.0   \n",
       "29882   BNTX 2020-12-30  83.38  85.50  80.5500  81.52       2607147.0   \n",
       "\n",
       "       Volume Weighted Average Price  Number of Transactions  \n",
       "0                            74.9395                  113015  \n",
       "1                            77.7958                  160884  \n",
       "2                            75.4936                  113523  \n",
       "3                            77.9694                  124950  \n",
       "4                            78.7228                  141919  \n",
       "...                              ...                     ...  \n",
       "29878                        97.8215                   14211  \n",
       "29879                        89.9943                   58940  \n",
       "29880                        87.1904                   35970  \n",
       "29881                        85.8906                   35970  \n",
       "29882                        82.1550                   31521  \n",
       "\n",
       "[13355 rows x 9 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train = dataset[dataset[\"Date\"].isin(pd.date_range(\"2020-03-11\", \"2020-12-30\"))]\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f39bb769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[73.28],\n",
       "       [77.91],\n",
       "       [70.65],\n",
       "       ...,\n",
       "       [84.45],\n",
       "       [90.  ],\n",
       "       [83.38]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set = dataset_train.iloc[:, 2:3].values\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b925797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "# Use Normalization (versus Standardization) for RNNs with Sigmoid Activation Functions\n",
    "# 'MinMaxScalar' is a Normalization Library\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# 'feature_range = (0,1)' makes sure that training data is scaled to have values between 0 and 1\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "training_set_scaled = sc.fit_transform(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0230bc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data structure with 60 timesteps (look back 60 days) and 1 output\n",
    "# This tells the RNN what to remember (Number of timesteps) when predicting the next Stock Price\n",
    "# The wrong number of timesteps can lead to Overfitting or bogus results\n",
    "# 'x_train' Input with 60 previous days' stock prices\n",
    "X_train = []\n",
    "# 'y_train' Output with next day's stock price\n",
    "y_train = []\n",
    "for i in range(60, 13355):\n",
    "    X_train.append(training_set_scaled[i-60:i, 0])\n",
    "    y_train.append(training_set_scaled[i, 0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a15e3895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Trading Volume</th>\n",
       "      <th>Volume Weighted Average Price</th>\n",
       "      <th>Number of Transactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABT</td>\n",
       "      <td>2020-03-11</td>\n",
       "      <td>73.28</td>\n",
       "      <td>78.15</td>\n",
       "      <td>72.0100</td>\n",
       "      <td>74.74</td>\n",
       "      <td>14123240.0</td>\n",
       "      <td>74.9395</td>\n",
       "      <td>113015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABT</td>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>77.91</td>\n",
       "      <td>82.67</td>\n",
       "      <td>74.8300</td>\n",
       "      <td>81.65</td>\n",
       "      <td>18521541.0</td>\n",
       "      <td>77.7958</td>\n",
       "      <td>160884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABT</td>\n",
       "      <td>2020-03-15</td>\n",
       "      <td>70.65</td>\n",
       "      <td>79.25</td>\n",
       "      <td>70.0000</td>\n",
       "      <td>73.66</td>\n",
       "      <td>14358978.0</td>\n",
       "      <td>75.4936</td>\n",
       "      <td>113523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABT</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>75.52</td>\n",
       "      <td>79.79</td>\n",
       "      <td>73.3400</td>\n",
       "      <td>79.49</td>\n",
       "      <td>13441141.0</td>\n",
       "      <td>77.9694</td>\n",
       "      <td>124950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABT</td>\n",
       "      <td>2020-03-17</td>\n",
       "      <td>74.98</td>\n",
       "      <td>81.99</td>\n",
       "      <td>74.3900</td>\n",
       "      <td>79.26</td>\n",
       "      <td>17680823.0</td>\n",
       "      <td>78.7228</td>\n",
       "      <td>141919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29878</th>\n",
       "      <td>BNTX</td>\n",
       "      <td>2020-12-23</td>\n",
       "      <td>98.50</td>\n",
       "      <td>99.89</td>\n",
       "      <td>96.6400</td>\n",
       "      <td>96.96</td>\n",
       "      <td>1039109.0</td>\n",
       "      <td>97.8215</td>\n",
       "      <td>14211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29879</th>\n",
       "      <td>BNTX</td>\n",
       "      <td>2020-12-27</td>\n",
       "      <td>95.00</td>\n",
       "      <td>95.40</td>\n",
       "      <td>87.0700</td>\n",
       "      <td>88.11</td>\n",
       "      <td>4698872.0</td>\n",
       "      <td>89.9943</td>\n",
       "      <td>58940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29880</th>\n",
       "      <td>BNTX</td>\n",
       "      <td>2020-12-28</td>\n",
       "      <td>84.45</td>\n",
       "      <td>89.39</td>\n",
       "      <td>83.8525</td>\n",
       "      <td>88.28</td>\n",
       "      <td>3300440.0</td>\n",
       "      <td>87.1904</td>\n",
       "      <td>35970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29881</th>\n",
       "      <td>BNTX</td>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>90.00</td>\n",
       "      <td>90.89</td>\n",
       "      <td>83.0268</td>\n",
       "      <td>83.94</td>\n",
       "      <td>2941083.0</td>\n",
       "      <td>85.8906</td>\n",
       "      <td>35970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29882</th>\n",
       "      <td>BNTX</td>\n",
       "      <td>2020-12-30</td>\n",
       "      <td>83.38</td>\n",
       "      <td>85.50</td>\n",
       "      <td>80.5500</td>\n",
       "      <td>81.52</td>\n",
       "      <td>2607147.0</td>\n",
       "      <td>82.1550</td>\n",
       "      <td>31521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13355 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Ticker       Date   Open   High      Low  Close  Trading Volume  \\\n",
       "0        ABT 2020-03-11  73.28  78.15  72.0100  74.74      14123240.0   \n",
       "1        ABT 2020-03-12  77.91  82.67  74.8300  81.65      18521541.0   \n",
       "2        ABT 2020-03-15  70.65  79.25  70.0000  73.66      14358978.0   \n",
       "3        ABT 2020-03-16  75.52  79.79  73.3400  79.49      13441141.0   \n",
       "4        ABT 2020-03-17  74.98  81.99  74.3900  79.26      17680823.0   \n",
       "...      ...        ...    ...    ...      ...    ...             ...   \n",
       "29878   BNTX 2020-12-23  98.50  99.89  96.6400  96.96       1039109.0   \n",
       "29879   BNTX 2020-12-27  95.00  95.40  87.0700  88.11       4698872.0   \n",
       "29880   BNTX 2020-12-28  84.45  89.39  83.8525  88.28       3300440.0   \n",
       "29881   BNTX 2020-12-29  90.00  90.89  83.0268  83.94       2941083.0   \n",
       "29882   BNTX 2020-12-30  83.38  85.50  80.5500  81.52       2607147.0   \n",
       "\n",
       "       Volume Weighted Average Price  Number of Transactions  \n",
       "0                            74.9395                  113015  \n",
       "1                            77.7958                  160884  \n",
       "2                            75.4936                  113523  \n",
       "3                            77.9694                  124950  \n",
       "4                            78.7228                  141919  \n",
       "...                              ...                     ...  \n",
       "29878                        97.8215                   14211  \n",
       "29879                        89.9943                   58940  \n",
       "29880                        87.1904                   35970  \n",
       "29881                        85.8906                   35970  \n",
       "29882                        82.1550                   31521  \n",
       "\n",
       "[13355 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reshaping (add more dimensions)\n",
    "# This lets you add more indicators that may potentially have corelation with Stock Prices\n",
    "# Keras RNNs expects an input shape (Batch Size, Timesteps, input_dim)\n",
    "# '.shape[0]' is the number of Rows (Batch Size)\n",
    "# '.shape[1]' is the number of Columns (timesteps)\n",
    "# 'input_dim' is the number of factors that may affect stock prices\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "# Show the dataset we're working with\n",
    "display(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0eca50ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 - Building the RNN\n",
    "# Building a robust stacked LSTM with dropout regularization\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "439bde68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the RNN\n",
    "# Regression is when you predict a continuous value\n",
    "regressor = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "30365ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the first LSTM layer and some Dropout regularisation\n",
    "# 'units' is the number of LSTM Memory Cells (Neurons) for higher dimensionality\n",
    "# 'return_sequences = True' because we will add more stacked LSTM Layers\n",
    "# 'input_shape' of x_train\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "# 20% of Neurons will be ignored (10 out of 50 Neurons) to prevent Overfitting\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "adfb4eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "# Not need to specify input_shape for second Layer, it knows that we have 50 Neurons from the previous layer\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "# This is the last LSTM Layer. 'return_sequences = false' by default so we leave it out.\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "01181d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "416/416 [==============================] - 30s 57ms/step - loss: 0.0024\n",
      "Epoch 2/100\n",
      "416/416 [==============================] - 25s 60ms/step - loss: 0.0016\n",
      "Epoch 3/100\n",
      "416/416 [==============================] - 26s 62ms/step - loss: 0.0010\n",
      "Epoch 4/100\n",
      "416/416 [==============================] - 26s 61ms/step - loss: 8.5707e-04\n",
      "Epoch 5/100\n",
      "416/416 [==============================] - 26s 62ms/step - loss: 7.5252e-04\n",
      "Epoch 6/100\n",
      "416/416 [==============================] - 24s 58ms/step - loss: 6.9543e-04\n",
      "Epoch 7/100\n",
      "416/416 [==============================] - 23s 55ms/step - loss: 7.3466e-04\n",
      "Epoch 8/100\n",
      "416/416 [==============================] - 23s 55ms/step - loss: 6.6242e-04\n",
      "Epoch 9/100\n",
      "416/416 [==============================] - 23s 56ms/step - loss: 5.8908e-04\n",
      "Epoch 10/100\n",
      "416/416 [==============================] - 23s 55ms/step - loss: 5.3183e-04\n",
      "Epoch 11/100\n",
      "416/416 [==============================] - 23s 56ms/step - loss: 5.3761e-04\n",
      "Epoch 12/100\n",
      "416/416 [==============================] - 23s 56ms/step - loss: 5.0848e-04\n",
      "Epoch 13/100\n",
      "416/416 [==============================] - 23s 54ms/step - loss: 4.3724e-04\n",
      "Epoch 14/100\n",
      "416/416 [==============================] - 22s 54ms/step - loss: 4.7022e-04\n",
      "Epoch 15/100\n",
      "416/416 [==============================] - 23s 54ms/step - loss: 6.2184e-04\n",
      "Epoch 16/100\n",
      "416/416 [==============================] - 23s 55ms/step - loss: 4.5653e-04\n",
      "Epoch 17/100\n",
      "416/416 [==============================] - 22s 54ms/step - loss: 4.9890e-04\n",
      "Epoch 18/100\n",
      "416/416 [==============================] - 22s 54ms/step - loss: 4.9505e-04\n",
      "Epoch 19/100\n",
      "416/416 [==============================] - 22s 54ms/step - loss: 5.0618e-04\n",
      "Epoch 20/100\n",
      "416/416 [==============================] - 22s 54ms/step - loss: 4.4518e-04\n",
      "Epoch 21/100\n",
      "416/416 [==============================] - 22s 54ms/step - loss: 4.4804e-04\n",
      "Epoch 22/100\n",
      "416/416 [==============================] - 22s 54ms/step - loss: 4.8119e-04\n",
      "Epoch 23/100\n",
      "416/416 [==============================] - 22s 54ms/step - loss: 4.5467e-04\n",
      "Epoch 24/100\n",
      "416/416 [==============================] - 23s 54ms/step - loss: 4.9758e-04\n",
      "Epoch 25/100\n",
      "416/416 [==============================] - 22s 54ms/step - loss: 4.6404e-04\n",
      "Epoch 26/100\n",
      "416/416 [==============================] - 22s 54ms/step - loss: 4.5556e-04\n",
      "Epoch 27/100\n",
      "416/416 [==============================] - 23s 54ms/step - loss: 4.0267e-04\n",
      "Epoch 28/100\n",
      "416/416 [==============================] - 24s 57ms/step - loss: 4.3543e-04\n",
      "Epoch 29/100\n",
      "416/416 [==============================] - 24s 57ms/step - loss: 4.3741e-04\n",
      "Epoch 30/100\n",
      "416/416 [==============================] - 26s 62ms/step - loss: 4.1564e-04\n",
      "Epoch 31/100\n",
      "416/416 [==============================] - 24s 57ms/step - loss: 4.3122e-04\n",
      "Epoch 32/100\n",
      "416/416 [==============================] - 24s 57ms/step - loss: 4.0724e-04\n",
      "Epoch 33/100\n",
      "416/416 [==============================] - 23s 55ms/step - loss: 4.2360e-04\n",
      "Epoch 34/100\n",
      "416/416 [==============================] - 22s 54ms/step - loss: 4.3139e-04\n",
      "Epoch 35/100\n",
      "416/416 [==============================] - 23s 54ms/step - loss: 4.2885e-04\n",
      "Epoch 36/100\n",
      "416/416 [==============================] - 22s 54ms/step - loss: 4.3881e-04\n",
      "Epoch 37/100\n",
      "416/416 [==============================] - 23s 55ms/step - loss: 4.5924e-04\n",
      "Epoch 38/100\n",
      "416/416 [==============================] - 23s 54ms/step - loss: 4.1758e-04\n",
      "Epoch 39/100\n",
      "416/416 [==============================] - 23s 55ms/step - loss: 4.4032e-04\n",
      "Epoch 40/100\n",
      "416/416 [==============================] - 22s 54ms/step - loss: 3.9352e-04\n",
      "Epoch 41/100\n",
      "416/416 [==============================] - 23s 54ms/step - loss: 4.1549e-04\n",
      "Epoch 42/100\n",
      "416/416 [==============================] - 23s 56ms/step - loss: 4.2092e-04\n",
      "Epoch 43/100\n",
      "416/416 [==============================] - 23s 54ms/step - loss: 3.9756e-04\n",
      "Epoch 44/100\n",
      "416/416 [==============================] - 22s 54ms/step - loss: 4.1015e-04\n",
      "Epoch 45/100\n",
      "416/416 [==============================] - 22s 54ms/step - loss: 4.0781e-04\n",
      "Epoch 46/100\n",
      "416/416 [==============================] - 23s 54ms/step - loss: 4.1403e-04\n",
      "Epoch 47/100\n",
      "416/416 [==============================] - 22s 54ms/step - loss: 4.0894e-04\n",
      "Epoch 48/100\n",
      "416/416 [==============================] - 23s 55ms/step - loss: 4.4498e-04\n",
      "Epoch 49/100\n",
      "416/416 [==============================] - 23s 54ms/step - loss: 3.9381e-04\n",
      "Epoch 50/100\n",
      "416/416 [==============================] - 22s 54ms/step - loss: 4.0334e-04\n",
      "Epoch 51/100\n",
      "416/416 [==============================] - 23s 54ms/step - loss: 4.0758e-04\n",
      "Epoch 52/100\n",
      "416/416 [==============================] - 23s 54ms/step - loss: 4.4078e-04\n",
      "Epoch 53/100\n",
      "416/416 [==============================] - 22s 54ms/step - loss: 4.2174e-04\n",
      "Epoch 54/100\n",
      "416/416 [==============================] - 23s 54ms/step - loss: 3.8201e-04\n",
      "Epoch 55/100\n",
      "416/416 [==============================] - 22s 54ms/step - loss: 4.2774e-04\n",
      "Epoch 56/100\n",
      "416/416 [==============================] - 22s 54ms/step - loss: 4.1697e-04\n",
      "Epoch 57/100\n",
      "416/416 [==============================] - 22s 54ms/step - loss: 3.8772e-04\n",
      "Epoch 58/100\n",
      "416/416 [==============================] - 22s 54ms/step - loss: 4.5002e-04\n",
      "Epoch 59/100\n",
      "416/416 [==============================] - 22s 54ms/step - loss: 3.8476e-04\n",
      "Epoch 60/100\n",
      "416/416 [==============================] - 22s 53ms/step - loss: 3.8671e-04\n",
      "Epoch 61/100\n",
      "416/416 [==============================] - 22s 54ms/step - loss: 4.1462e-04\n",
      "Epoch 62/100\n",
      "416/416 [==============================] - 22s 54ms/step - loss: 4.0826e-04\n",
      "Epoch 63/100\n",
      "416/416 [==============================] - 22s 54ms/step - loss: 4.0751e-04\n",
      "Epoch 64/100\n",
      "416/416 [==============================] - 22s 54ms/step - loss: 3.7372e-04\n",
      "Epoch 65/100\n",
      "416/416 [==============================] - 23s 54ms/step - loss: 4.2003e-04\n",
      "Epoch 66/100\n",
      "416/416 [==============================] - 22s 53ms/step - loss: 3.9515e-04\n",
      "Epoch 67/100\n",
      "416/416 [==============================] - 24s 57ms/step - loss: 4.2505e-04\n",
      "Epoch 68/100\n",
      "416/416 [==============================] - 23s 56ms/step - loss: 3.8102e-04\n",
      "Epoch 69/100\n",
      "416/416 [==============================] - 28s 67ms/step - loss: 3.8267e-04\n",
      "Epoch 70/100\n",
      "416/416 [==============================] - 24s 58ms/step - loss: 4.1612e-04\n",
      "Epoch 71/100\n",
      "416/416 [==============================] - 24s 57ms/step - loss: 3.6931e-04\n",
      "Epoch 72/100\n",
      "416/416 [==============================] - 23s 56ms/step - loss: 3.8278e-04\n",
      "Epoch 73/100\n",
      "416/416 [==============================] - 24s 57ms/step - loss: 4.0543e-04\n",
      "Epoch 74/100\n",
      "416/416 [==============================] - 24s 57ms/step - loss: 3.9733e-04\n",
      "Epoch 75/100\n",
      "416/416 [==============================] - 24s 57ms/step - loss: 3.8780e-04\n",
      "Epoch 76/100\n",
      "416/416 [==============================] - 24s 57ms/step - loss: 3.8663e-04\n",
      "Epoch 77/100\n",
      "416/416 [==============================] - 24s 57ms/step - loss: 3.8173e-04\n",
      "Epoch 78/100\n",
      "416/416 [==============================] - 24s 58ms/step - loss: 4.0441e-04\n",
      "Epoch 79/100\n",
      "416/416 [==============================] - 24s 57ms/step - loss: 3.6542e-04\n",
      "Epoch 80/100\n",
      "416/416 [==============================] - 24s 57ms/step - loss: 4.1994e-04\n",
      "Epoch 81/100\n",
      "416/416 [==============================] - 24s 57ms/step - loss: 3.9059e-04\n",
      "Epoch 82/100\n",
      "416/416 [==============================] - 24s 57ms/step - loss: 4.0105e-04\n",
      "Epoch 83/100\n",
      "416/416 [==============================] - 23s 56ms/step - loss: 3.9352e-04\n",
      "Epoch 84/100\n",
      "416/416 [==============================] - 24s 57ms/step - loss: 4.0482e-04\n",
      "Epoch 85/100\n",
      "416/416 [==============================] - 24s 57ms/step - loss: 3.7956e-04\n",
      "Epoch 86/100\n",
      "416/416 [==============================] - 23s 56ms/step - loss: 3.8294e-04\n",
      "Epoch 87/100\n",
      "416/416 [==============================] - 24s 57ms/step - loss: 3.9639e-04\n",
      "Epoch 88/100\n",
      "416/416 [==============================] - 24s 57ms/step - loss: 3.6639e-04\n",
      "Epoch 89/100\n",
      "416/416 [==============================] - 24s 57ms/step - loss: 3.7085e-04\n",
      "Epoch 90/100\n",
      "416/416 [==============================] - 24s 57ms/step - loss: 3.9497e-04\n",
      "Epoch 91/100\n",
      "416/416 [==============================] - 24s 57ms/step - loss: 3.8797e-04\n",
      "Epoch 92/100\n",
      "416/416 [==============================] - 23s 56ms/step - loss: 3.8085e-04\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416/416 [==============================] - 23s 56ms/step - loss: 3.8509e-04\n",
      "Epoch 94/100\n",
      "416/416 [==============================] - 23s 56ms/step - loss: 3.8286e-04\n",
      "Epoch 95/100\n",
      "416/416 [==============================] - 23s 56ms/step - loss: 3.7126e-04\n",
      "Epoch 96/100\n",
      "416/416 [==============================] - 23s 56ms/step - loss: 3.8238e-04\n",
      "Epoch 97/100\n",
      "416/416 [==============================] - 23s 56ms/step - loss: 3.7538e-04\n",
      "Epoch 98/100\n",
      "416/416 [==============================] - 23s 56ms/step - loss: 3.4427e-04\n",
      "Epoch 99/100\n",
      "416/416 [==============================] - 23s 56ms/step - loss: 3.6667e-04\n",
      "Epoch 100/100\n",
      "416/416 [==============================] - 23s 56ms/step - loss: 4.4291e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9ae1b8d550>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding the output layer\n",
    "# 'units = 1' because Output layer has one dimension\n",
    "regressor.add(Dense(units = 1))\n",
    "\n",
    "# Compiling the RNN\n",
    "# Keras documentation recommends 'RMSprop' as a good optimizer for RNNs\n",
    "# Trial and error suggests that 'adam' optimizer is a good choice\n",
    "# loss = 'mean_squared_error' which is good for Regression vs. 'Binary Cross Entropy' previously used for Classification\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "# Fitting the RNN to the Training set\n",
    "# 'X_train' Independent variables\n",
    "# 'y_train' Output Truths that we compare X_train to.\n",
    "regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c8feb20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3 - Making the predictions and visualising the results\n",
    "\n",
    "# Getting the real stock price of 2021\n",
    "dataset = pd.read_csv(\"Datasets/SPX_Health_Sector_stocks_all.csv\")\n",
    "\n",
    "dataset['Date'] =  pd.to_datetime(dataset['Date'], infer_datetime_format=True)\n",
    "dataset_test = dataset[dataset[\"Date\"].isin(pd.date_range(\"2021-01-01\", \"2021-12-30\"))]\n",
    "real_stock_price = dataset_test.iloc[:, 2:3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "113701e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted stock price of 2021\n",
    "# We need 60 previous inputs for each day of the Test_set in 2021\n",
    "# Combine 'dataset_train' and 'dataset_test'\n",
    "# 'axis = 0' for Vertical Concatenation to add rows to the bottom\n",
    "dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "590ccadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Stock Prices for Test time period, plus 60 days previous\n",
    "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values\n",
    "# 'reshape' function to get it into a NumPy format\n",
    "inputs = inputs.reshape(-1,1)\n",
    "# Inputs need to be scaled to match the model trained on Scaled Feature\n",
    "inputs = sc.transform(inputs)\n",
    "# The following is pasted from above and modified for Testing, romove all 'Ys'\n",
    "X_test = []\n",
    "\n",
    "for i in range(60, 16840):\n",
    "    X_test.append(inputs[i-60:i, 0])\n",
    "\n",
    "X_test = np.array(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f8ee7c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a 3D input so add another dimension\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "# Predict the Stock Price\n",
    "predicted_stock_price = regressor.predict(X_test)\n",
    "# We need to inverse the scaling of our prediction to get a Dollar amount\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2b81d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5547fc13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
