{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "loading_csv_to_db",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrJYhkrcouQ6",
        "outputId": "4c1af601-fed9-4e81-a62a-bc8158c18b60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rIgn:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [76.0 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:13 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [872 kB]\n",
            "Get:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [738 kB]\n",
            "Hit:18 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,516 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,463 kB]\n",
            "Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,823 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,242 kB]\n",
            "Get:23 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [934 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,954 kB]\n",
            "Get:25 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [771 kB]\n",
            "Fetched 14.7 MB in 5s (3,180 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.0 from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-3.0.3'\n",
        "spark_version = 'spark-2.4.8'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://jdbc.postgresql.org/download/postgresql-42.2.16.jar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOg3xzY0pO1W",
        "outputId": "10a7f713-2cfb-4727-ab62-e209b716d690"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-23 07:17:50--  https://jdbc.postgresql.org/download/postgresql-42.2.16.jar\n",
            "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
            "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1002883 (979K) [application/java-archive]\n",
            "Saving to: ‘postgresql-42.2.16.jar’\n",
            "\n",
            "postgresql-42.2.16. 100%[===================>] 979.38K  5.92MB/s    in 0.2s    \n",
            "\n",
            "2022-01-23 07:17:51 (5.92 MB/s) - ‘postgresql-42.2.16.jar’ saved [1002883/1002883]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Postgres driver that will allow Spark to interact with Postgres.\n",
        "!wget https://jdbc.postgresql.org/download/postgresql-42.2.16.jar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihU5-vZ2psEq",
        "outputId": "8a30dde9-1639-4f23-f730-dd344c609a38"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-23 07:17:51--  https://jdbc.postgresql.org/download/postgresql-42.2.16.jar\n",
            "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
            "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1002883 (979K) [application/java-archive]\n",
            "Saving to: ‘postgresql-42.2.16.jar.1’\n",
            "\n",
            "postgresql-42.2.16. 100%[===================>] 979.38K  6.05MB/s    in 0.2s    \n",
            "\n",
            "2022-01-23 07:17:51 (6.05 MB/s) - ‘postgresql-42.2.16.jar.1’ saved [1002883/1002883]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"CloudETL\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.16.jar\").getOrCreate()"
      ],
      "metadata": {
        "id": "DlM9ExbcH0ou"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkFiles\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "\n",
        "urlList = [\"https://group8-bucket.s3.amazonaws.com/SPX_Health_Sector_stocks_all.csv\",\n",
        "\"https://group8-bucket.s3.amazonaws.com/SPX_Health_Sector_stocks.csv\",\n",
        "\"https://group8-bucket.s3.amazonaws.com/Covid_Companies_Stocks.csv\",\n",
        "\"https://group8-bucket.s3.amazonaws.com/SPX_Health_Sector_info.csv\",\n",
        "\"https://group8-bucket.s3.amazonaws.com/sp500indexand11sectors/S5COND_INDEX.csv\",\n",
        "\"https://group8-bucket.s3.amazonaws.com/sp500indexand11sectors/S5CONS_INDEX.csv\",\n",
        "\"https://group8-bucket.s3.amazonaws.com/sp500indexand11sectors/S5ENRS_INDEX.csv\",\n",
        "\"https://group8-bucket.s3.amazonaws.com/sp500indexand11sectors/S5FINL_INDEX.csv\",\n",
        "\"https://group8-bucket.s3.amazonaws.com/sp500indexand11sectors/S5HLTH_INDEX.csv\",\n",
        "\"https://group8-bucket.s3.amazonaws.com/sp500indexand11sectors/S5INDU_INDEX.csv\",\n",
        "\"https://group8-bucket.s3.amazonaws.com/sp500indexand11sectors/S5INFT_INDEX.csv\",\n",
        "\"https://group8-bucket.s3.amazonaws.com/sp500indexand11sectors/S5MATR_INDEX.csv\",\n",
        "\"https://group8-bucket.s3.amazonaws.com/sp500indexand11sectors/S5RLST_Index.csv\",\n",
        "\"https://group8-bucket.s3.amazonaws.com/sp500indexand11sectors/S5TELS_INDEX.csv\",\n",
        "\"https://group8-bucket.s3.amazonaws.com/sp500indexand11sectors/S5UTIL_INDEX.csv\",\n",
        "\"https://group8-bucket.s3.amazonaws.com/sp500indexand11sectors/SPX_INDEX.csv\" ]\n",
        "\n",
        "for x in urlList:\n",
        "   spark.sparkContext.addFile(x)\n",
        "\n",
        "#ToDo convert to loop\n",
        "user_data_df =  spark.read.csv(SparkFiles.get(\"SPX_Health_Sector_stocks_all.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "user_data1_df = spark.read.csv(SparkFiles.get(\"SPX_Health_Sector_stocks.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "user_data2_df = spark.read.csv(SparkFiles.get(\"Covid_Companies_Stocks.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "user_data3_df = spark.read.csv(SparkFiles.get(\"SPX_Health_Sector_info.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "user_data4_df = spark.read.csv(SparkFiles.get(\"S5COND_INDEX.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "user_data5_df = spark.read.csv(SparkFiles.get(\"S5CONS_INDEX.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "user_data6_df = spark.read.csv(SparkFiles.get(\"S5ENRS_INDEX.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "user_data7_df = spark.read.csv(SparkFiles.get(\"S5FINL_INDEX.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "user_data8_df = spark.read.csv(SparkFiles.get(\"S5HLTH_INDEX.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "user_data9_df = spark.read.csv(SparkFiles.get(\"S5INDU_INDEX.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "user_data10_df = spark.read.csv(SparkFiles.get(\"S5INFT_INDEX.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "user_data11_df = spark.read.csv(SparkFiles.get(\"S5MATR_INDEX.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "user_data12_df = spark.read.csv(SparkFiles.get(\"S5RLST_Index.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "user_data13_df = spark.read.csv(SparkFiles.get(\"S5TELS_INDEX.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "user_data14_df = spark.read.csv(SparkFiles.get(\"S5UTIL_INDEX.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "user_data15_df = spark.read.csv(SparkFiles.get(\"SPX_INDEX.csv\"), sep=\",\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "Ue1qJfpqIF2Q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#user_data_df['Date'] = pd.to_datetime(user_data_df['Date'], format=\"%y/%m/%d\")\n",
        "from pyspark.sql.functions import to_date\n",
        "\n",
        "#ToDo: convert to loop\n",
        "user_data_df = user_data_df.select(['Ticker', to_date('Date', 'YYY-MM-dd').alias(\"date\"),'Open', 'High', 'Low', 'Close','Trading_Volume', 'Volume_Weighted_Average_Price' ,'Number_of_Transactions'])\n",
        "user_data1_df = user_data1_df.select(['Ticker', to_date('Date', 'YYY-MM-dd').alias(\"date\"),'Open', 'High', 'Low', 'Close','Trading_Volume', 'Volume_Weighted_Average_Price' ,'Number_of_Transactions'])\n",
        "user_data2_df = user_data2_df.select(['Ticker', to_date('Date', 'YYY-MM-dd').alias(\"date\"),'Open', 'High', 'Low', 'Close','Trading_Volume', 'Volume_Weighted_Average_Price' ,'Number_of_Transactions'])\n",
        "user_data3_df = user_data3_df.select(['Ticker','Company', 'Sector'])\n",
        "user_data4_df = user_data4_df.select([to_date('Date', 'YYYY-MM-dd').alias(\"date\"), 'Open' , 'High', 'Low', 'Close'])\n",
        "user_data5_df = user_data5_df.select([to_date('Date', 'YYYY-MM-dd').alias(\"date\"), 'Open' , 'High', 'Low', 'Close'])\n",
        "user_data6_df = user_data6_df.select([to_date('Date', 'YYYY-MM-dd').alias(\"date\"), 'Open' , 'High', 'Low', 'Close'])\n",
        "user_data7_df = user_data7_df.select([to_date('Date', 'YYYY-MM-dd').alias(\"date\"), 'Open' , 'High', 'Low', 'Close'])\n",
        "user_data8_df = user_data8_df.select([to_date('Date', 'YYYY-MM-dd').alias(\"date\"), 'Open' , 'High', 'Low', 'Close'])\n",
        "user_data9_df = user_data9_df.select([to_date('Date', 'YYYY-MM-dd').alias(\"date\"), 'Open' , 'High', 'Low', 'Close'])\n",
        "user_data10_df = user_data10_df.select([to_date('Date', 'YYYY-MM-dd').alias(\"date\"), 'Open' , 'High', 'Low', 'Close'])\n",
        "user_data11_df = user_data11_df.select([to_date('Date', 'YYYY-MM-dd').alias(\"date\"), 'Open' , 'High', 'Low', 'Close'])\n",
        "user_data12_df = user_data12_df.select([to_date('Date', 'YYYY-MM-dd').alias(\"date\"), 'Open' , 'High', 'Low', 'Close'])\n",
        "user_data13_df = user_data13_df.select([to_date('Date', 'YYYY-MM-dd').alias(\"date\"), 'Open' , 'High', 'Low', 'Close'])\n",
        "user_data14_df = user_data14_df.select([to_date('Date', 'YYYY-MM-dd').alias(\"date\"), 'Open' , 'High', 'Low', 'Close'])\n",
        "user_data15_df = user_data15_df.select([to_date('Date', 'YYYY-MM-dd').alias(\"date\"), 'Open' , 'High', 'Low', 'Close'])"
      ],
      "metadata": {
        "id": "1VzNNeqN4gqY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store environmental variable\n",
        "from getpass import getpass\n",
        "password = getpass('Enter database password')\n",
        "# Configure settings for RDS\n",
        "mode = \"append\"\n",
        "jdbc_url=\"jdbc:postgresql://database-1.cxmneo38riye.us-east-2.rds.amazonaws.com:5432/postgres\"\n",
        "config = {\"user\":\"postgres\",\n",
        "          \"password\": password,\n",
        "          \"driver\":\"org.postgresql.Driver\"}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTKu-4eQLaKf",
        "outputId": "04955799-844f-4a1d-d2f1-c162060ad520"
      },
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter database password··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write user_data_df to table in RDS\n",
        "\n",
        "#ToDo Convert to for loop\n",
        "user_data_df.write.jdbc(url=jdbc_url, table='spx_health_sector_stocks_all', mode=mode, properties=config)\n",
        "user_data1_df.write.jdbc(url=jdbc_url, table='spx_health_sector_stocks', mode=mode, properties=config)\n",
        "user_data2_df.write.jdbc(url=jdbc_url, table='covid_companies_stocks', mode=mode, properties=config)\n",
        "user_data3_df.write.jdbc(url=jdbc_url, table='spx_health_sector_Info', mode=mode, properties=config)\n",
        "user_data4_df.write.jdbc(url=jdbc_url, table='s5cond_index', mode=mode, properties=config)\n",
        "user_data5_df.write.jdbc(url=jdbc_url, table='s5cons_index', mode=mode, properties=config)\n",
        "user_data6_df.write.jdbc(url=jdbc_url, table='s5enrs_index', mode=mode, properties=config)\n",
        "user_data7_df.write.jdbc(url=jdbc_url, table='s5finl_index', mode=mode, properties=config)\n",
        "user_data8_df.write.jdbc(url=jdbc_url, table='s5hlth_index', mode=mode, properties=config)\n",
        "user_data9_df.write.jdbc(url=jdbc_url, table='s5indu_index', mode=mode, properties=config)\n",
        "user_data10_df.write.jdbc(url=jdbc_url, table='s5inft_index', mode=mode, properties=config)\n",
        "user_data11_df.write.jdbc(url=jdbc_url, table='s5matr_index', mode=mode, properties=config)\n",
        "user_data12_df.write.jdbc(url=jdbc_url, table='s5rlst_index', mode=mode, properties=config)\n",
        "user_data13_df.write.jdbc(url=jdbc_url, table='s5tels_index', mode=mode, properties=config)\n",
        "user_data14_df.write.jdbc(url=jdbc_url, table='s5util_index', mode=mode, properties=config)\n",
        "user_data15_df.write.jdbc(url=jdbc_url, table='spx_index', mode=mode, properties=config)\n"
      ],
      "metadata": {
        "id": "0xRVObT3AFSK"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}